{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff63794c-34e0-47db-b585-c801583699e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd4724fc-771f-4315-aee8-5a380a835558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to load Python extension for LZ4 support. LZ4 compression will not be available.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "from bagpy import bagreader\n",
    "import rosbag\n",
    "import pandas as pd\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5c6e5ba-f44f-401e-bcbd-25a97454f1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3h9hh99m'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.util.generate_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "545632d7-8d02-4a41-b5d7-636c1935de0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_id = '29ivurzi'\n",
    "# run = wandb.init(project=\"screwing_estimation\", entity=\"serialexperimentsleon\", group=group_id)\n",
    "# run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "335a71af-8576-4c49-aff7-a0110b011178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb507622-8aa7-4eb6-aad5-d50f8483640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from screwing_dataset import ScrewingDataset\n",
    "from screwing_model import ScrewingModel, ScrewingModelSeq, ScrewingModelGaussian\n",
    "# from screwing_model_seq import \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69f71876-1eef-46e3-a3ef-0ac6c93beaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b9179ce-1265-4689-a43a-ffd87c8a08e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "413ed73a-26e5-4d17-b49a-a37f5024626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dset_dir = os.path.expanduser('~/datasets/screwing')\n",
    "\n",
    "# xprmnt_dir = time.strftime(\"/%Y-%m-%d_%H-%M-%S\")\n",
    "xprmnt_dir = time.strftime(\"/2022-04-14_15-49-01\")\n",
    "# xprmnt_dir = time.strftime(\"/test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed62ff9e-3298-458a-a30f-b6ee16af9c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/serialexperimentsleon/datasets/screwing/2022-04-14_15-49-01/4_2022-04-14-15-50-27.bag\n",
      "[INFO]  Data folder /home/serialexperimentsleon/datasets/screwing/2022-04-14_15-49-01/4_2022-04-14-15-50-27 already exists. Not creating.\n",
      "206\n"
     ]
    }
   ],
   "source": [
    "bag_path_names = base_dset_dir + xprmnt_dir + '/' + str(4) + '_*.bag' \n",
    "# print(bag_path_names)\n",
    "bag_path = glob.glob(bag_path_names)[0]\n",
    "print(bag_path)\n",
    "# bag = rosbag.Bag(bag_path)\n",
    "breader = bagreader(bag_path)\n",
    "bag = rosbag.Bag(bag_path)\n",
    "\n",
    "main_topic = '/panda/franka_state_controller_custom/franka_states'\n",
    "main_num_msgs = bag.get_message_count(main_topic)\n",
    "\n",
    "print(main_num_msgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9af95dc1-7572-49f4-9674-f6703c1ef3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/serialexperimentsleon/datasets/screwing/2022-04-14_15-49-01/0_2022-04-14-15-49-06.bag\n",
      "[INFO]  Data folder /home/serialexperimentsleon/datasets/screwing/2022-04-14_15-49-01/0_2022-04-14-15-49-06 already exists. Not creating.\n",
      "/home/serialexperimentsleon/datasets/screwing/2022-04-14_15-49-01/1_2022-04-14-15-49-58.bag\n",
      "[INFO]  Data folder /home/serialexperimentsleon/datasets/screwing/2022-04-14_15-49-01/1_2022-04-14-15-49-58 already exists. Not creating.\n"
     ]
    }
   ],
   "source": [
    "dset_list = []\n",
    "window_size = 10\n",
    "num_dset = 2\n",
    "for i in range(num_dset):\n",
    "    id_str = str(i)\n",
    "    bag_path_names = base_dset_dir + xprmnt_dir + '/' + id_str + '_*.bag' \n",
    "    bag_path = glob.glob(bag_path_names)[0]\n",
    "    print(bag_path)\n",
    "\n",
    "    pos_path_name = base_dset_dir + xprmnt_dir + '/' + id_str + '_pos.npy'\n",
    "    proj_ori_path = base_dset_dir + xprmnt_dir + '/' + id_str + '_proj_ori.npy'\n",
    "    pos_ori_path_list = [pos_path_name, proj_ori_path]\n",
    "    \n",
    "    dset_list.append(ScrewingDataset(bag_path, pos_ori_path_list, window_size, overlapping=True))\n",
    "    \n",
    "concat_dset = ConcatDataset(dset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "649a2772-b317-420a-8b0c-4ea3db8ec075",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "train_lder = DataLoader(\n",
    "    concat_dset,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# test_lder = DataLoader(\n",
    "#     test_dset,\n",
    "#     shuffle=True,\n",
    "#     num_workers=2,\n",
    "#     batch_size=batch_size\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "895938f5-66c7-4e0e-83d5-cfdee06c66cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ScrewingModelGaussian(\n",
       "  (lstm): LSTM(19, 10, num_layers=3, batch_first=True)\n",
       "  (hidden2out): Linear(in_features=10, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 19\n",
    "hidden_dim = 10\n",
    "num_layers = 3\n",
    "output_dim = 5\n",
    "model = ScrewingModelGaussian(input_dim, hidden_dim, num_layers, output_dim)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "677dd14b-91e9-40b4-a0d3-d391f5493aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import GNLL_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7b021014-7276-47d0-9f22-6caae1bf7b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5596,  0.0032,  0.5035, -0.2501,  0.0369],\n",
      "        [ 0.5500, -0.0113,  0.5035,  0.0594,  0.0219]], device='cuda:0')\n",
      "tensor([[-0.1676, -0.1707, -0.1165, -0.3216, -0.0952,  1.1079,  1.1056,  1.2453,\n",
      "          0.7929,  1.2192],\n",
      "        [-0.1875, -0.1855, -0.0759, -0.2852, -0.0658,  1.1472,  1.0395,  1.2181,\n",
      "          0.8046,  1.1991]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor(5.2179, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.6427, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ori_rel_weight = 1\n",
    "for batch_idx, return_dict in enumerate(train_lder):\n",
    "    x = return_dict['poses_wrenches_actions_tensor'].to(device)\n",
    "    y = return_dict['target'].float().to(device)\n",
    "    print(y)\n",
    "    # Forward propogation happens here\n",
    "    output = model(x).to(device) \n",
    "    print(output)\n",
    "    train_loss, pos_loss_ratio = GNLL_loss(output[:, :5], output[:, 5:], y, ori_rel_weight, log_pos_ratio=True)\n",
    "    print(train_loss)\n",
    "    print(pos_loss_ratio)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df30c12f-3508-4027-b517-cc9163825f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_metrics(model, ori_rel_weight, seq_length, val_loader): #TODO add early stopping criterion\n",
    "    # logging_step = 0\n",
    "    # quantiles of interest: median and 95% CI\n",
    "    q = torch.as_tensor([0.025, 0.5, 0.975]).to(device) \n",
    "    q_timing = torch.as_tensor([0.0, 0.2, 0.4, 0.6, 0.8, 1.0]).to(device)\n",
    "    seq_length = seq_length\n",
    "    ## switch model to eval\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for t in range(seq_length):\n",
    "\n",
    "            total_valid_pos_error, total_valid_ori_error, total_valid_loss = [], [], []\n",
    "            for batch_idx,(x,y, t) in enumerate(val_loader):\n",
    "                x = x.to(device)\n",
    "                y = y.float().to(device)\n",
    "\n",
    "                # Forward propogation happens here\n",
    "                outputs = model(x).to(device) \n",
    "                output_t = outputs[:, t, :]\n",
    "\n",
    "                loss = weighted_MSE_loss(output_t, y, ori_rel_weight)\n",
    "\n",
    "                ## evaluate and append analysis metrics\n",
    "                total_valid_ori_error.append(batched_ori_err(output_t, y))\n",
    "                total_valid_pos_error.append(batched_pos_err(output_t, y))\n",
    "                total_valid_loss.append(loss)\n",
    "\n",
    "                # if batch_idx % log_interval == 0:\n",
    "                #     wandb.log({\"loss\": loss, 'epoch': epoch, 'batch_idx': batch_idx})\n",
    "                \n",
    "                \n",
    "\n",
    "            total_valid_pos_error = torch.cat(total_valid_pos_error).to(device)\n",
    "            total_valid_ori_error = torch.cat(total_valid_ori_error).to(device)\n",
    "            total_valid_loss = torch.as_tensor(total_valid_loss).to(device)\n",
    "\n",
    "            ## statistical metrics from the test evaluations\n",
    "\n",
    "            ## pos error\n",
    "            pos_err_mean = torch.mean(total_valid_pos_error)\n",
    "            pos_err_std = torch.std(total_valid_pos_error)\n",
    "            pos_err_max = torch.max(total_valid_pos_error)\n",
    "            pos_err_min = torch.min(total_valid_pos_error)\n",
    "\n",
    "            ## 95% confidence interval and median\n",
    "            # q = torch.as_tensor([0.025, 0.5, 0.975]) \n",
    "            pos_err_95_median = torch.quantile(total_valid_pos_error, q, dim=0, keepdim=False, interpolation='nearest')\n",
    "\n",
    "            ## ori error\n",
    "            ori_err_mean = torch.mean(total_valid_ori_error)\n",
    "            ori_err_std = torch.std(total_valid_ori_error)\n",
    "            ori_err_max = torch.max(total_valid_ori_error)\n",
    "            ori_err_min = torch.min(total_valid_ori_error)\n",
    "\n",
    "            ## 95% confidence interval\n",
    "            ori_err_95_median = torch.quantile(total_valid_ori_error, q, dim=0, keepdim=False, interpolation='nearest')\n",
    "\n",
    "            ## loss \n",
    "            loss_mean = torch.mean(total_valid_loss)\n",
    "            loss_std = torch.std(total_valid_loss)\n",
    "            loss_max = torch.max(total_valid_loss)\n",
    "            loss_min = torch.min(total_valid_loss)\n",
    "\n",
    "            ## 95% confidence interval\n",
    "            loss_95_median = torch.quantile(total_valid_loss, q, dim=0, keepdim=False, interpolation='nearest')\n",
    "\n",
    "            wandb.log({ \n",
    "            'valid_pos_err_mean_' + str(t) : pos_err_mean,\n",
    "            'valid_pos_err_std_' + str(t) : pos_err_std,\n",
    "            'valid_pos_err_max_' + str(t) : pos_err_max,\n",
    "            'valid_pos_err_min_' + str(t) : pos_err_min,\n",
    "            'valid_pos_err_95_lower_' + str(t) : pos_err_95_median[0].item(),\n",
    "            'valid_pos_err_median_' + str(t) : pos_err_95_median[1].item(),\n",
    "            'valid_pos_err_95_upper_' + str(t) : pos_err_95_median[2].item(),\n",
    "            'valid_ori_err_mean_' + str(t) : ori_err_mean,\n",
    "            'valid_ori_err_std_' + str(t) : ori_err_std,\n",
    "            'valid_ori_err_max_' + str(t) : ori_err_max,\n",
    "            'valid_ori_err_min_' + str(t) : ori_err_min,\n",
    "            'valid_ori_err_95_lower_' + str(t) : ori_err_95_median[0].item(),\n",
    "            'valid_ori_err_median_' + str(t) : ori_err_95_median[1].item(),\n",
    "            'valid_ori_err_95_upper_' + str(t) : ori_err_95_median[2].item(),\n",
    "            'valid_loss_mean_' + str(t) : loss_mean,\n",
    "            'valid_loss_std_' + str(t) : loss_std,\n",
    "            'valid_loss_max_' + str(t) : loss_max,\n",
    "            'valid_loss_min_' + str(t) : loss_min,\n",
    "            'valid_loss_95_lower_' + str(t) : loss_95_median[0].item(),\n",
    "            'valid_loss_median_' + str(t) : loss_95_median[1].item(),\n",
    "            'valid_loss_95_upper_' + str(t) : loss_95_median[2].item()\n",
    "            }, step = logging_step-1)\n",
    "            ## log some summary metrics from the validation/eval run\n",
    "\n",
    "            ## log a figure of model output  \n",
    "\n",
    "        ## switch model back to train\n",
    "        model.train()\n",
    "             \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5c62e147-d1a2-4e71-b9f3-08ef7eb91acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(net,optimizer,criterion,num_epochs,train_loader):\n",
    "  overall_step = 0\n",
    "  counter = 0\n",
    "  loss_plot = []\n",
    "  for epoch in range(num_epochs):\n",
    "    for i,(x,y) in enumerate(train_loader):\n",
    "      counter += 1\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      x = x.to(device)\n",
    "      y = y.float().to(device)\n",
    "\n",
    "      # Forward propogation happens here\n",
    "      outputs = net(x).to(device) \n",
    "\n",
    "      # The following lines of code is where the magic happens!\n",
    "      # The partial derivatives of the Loss with respect to the \n",
    "      # weights are calculated and the weights are updated\n",
    "      loss = criterion(outputs,y)  \n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      loss_plot.append(loss.item())\n",
    "\n",
    "      # if (counter+1) % 10 == 0: \n",
    "          # argmax = torch.max(outputs, 1)[1]\n",
    "          # accuracy = (y == argmax.squeeze()).float().mean()\n",
    "\n",
    "#           info = {'loss' : loss.item()}\n",
    "#           for tag, value in info.items():\n",
    "#               logger.scalar_summary(tag, value, overall_step)\n",
    "#           overall_step+=1\n",
    "\n",
    "#           print ('Epoch : %d/%d, Iter : %d/%d,  Loss: %.4f' \n",
    "#                  %(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
    "  return net #, loss_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "8a3f0881-48cb-44a4-96f6-dd79e70f8e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 0.003\n",
    "\n",
    "# net = My_Neural_Network()\n",
    "# net = net.to(device)\n",
    "\n",
    "# model = ScrewingModel(input_dim, hidden_dim, ouput_dim) # no specification of \n",
    "model = model.to(device)\n",
    "loss_function = nn.MSELoss(reduction='sum') #alternatively mean the squared error or none ...\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "8ae244e2-ee66-4447-9035-dd9e86bfab16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4243, 0.1488, 0.7550, 0.0404, 0.7973],\n",
      "        [0.7016, 0.0033, 0.2402, 0.8475, 0.3636]])\n",
      "tensor([1.1872, 1.1833])\n",
      "tensor([[1.],\n",
      "        [1.]])\n",
      "tensor([[0.9657, 0.3413, 0.5083, 0.4488, 0.7348],\n",
      "        [0.2556, 0.4523, 0.4474, 0.1619, 0.5287]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4394, 0.4359, 1.4852, 0.0901, 1.0850],\n",
       "        [2.7447, 0.0073, 0.5368, 5.2353, 0.6876]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2,5)\n",
    "print(a)\n",
    "print(torch.norm(a, 2, 1))\n",
    "ones = torch.ones(2, 1)\n",
    "print(ones)\n",
    "torch.cat((a, ones), 1)\n",
    "\n",
    "b  = torch.rand(2,5)\n",
    "print(b)\n",
    "a / b\n",
    "# a.size()[0]\n",
    "# len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cf19125f-3697-4150-a4c6-95fb810f7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_MSE_loss(input, target, ori_weight):\n",
    "    SE_out = (input - target)**2\n",
    "    # print(torch.mean(SE_out))\n",
    "    # print(torch.sum(SE_out)/torch.numel(input))\n",
    "    pos_SE = SE_out[:, :3]\n",
    "    ori_SE = SE_out[:, 3:]\n",
    "    # print((torch.sum(pos_SE) + torch.sum(ori_SE))/torch.numel(input))\n",
    "    weighted_mean = (torch.sum(pos_SE) + ori_weight*torch.sum(ori_SE))/len(SE_out)\n",
    "    # weighted_sum = torch.sum(pos_SE) + ori_weight*torch.sum(ori_SE)\n",
    "    return weighted_mean\n",
    "    # return torch.sum(torch.mul(weighting, MSE_out))\n",
    "    # return torch.sum(MSE_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8648e3d2-b3d5-4d82-8f09-ad97ca7cebc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0254, -0.1393,  0.1538, -0.1316,  0.0819],\n",
      "        [ 0.0253, -0.1405,  0.1519, -0.1305,  0.0827]], device='cuda:0')\n",
      "tensor([[ 0.5526, -0.0125,  0.5124, -0.0292, -0.0315],\n",
      "        [ 0.5529, -0.0057,  0.5125, -0.0111, -0.0009]], device='cuda:0')\n",
      "tensor([0.0959, 0.0940], device='cuda:0')\n",
      "torch.Size([2])\n",
      "tensor([0.2603, 0.2594], device='cuda:0')\n",
      "torch.Size([2])\n",
      "tensor([0.0959, 0.0940, 0.2603, 0.2594], device='cuda:0')\n",
      "tensor([0.0959, 0.0940, 0.2603, 0.2594], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i, (x, y) in enumerate(train_lder):\n",
    "        # print(x.size())\n",
    "        x = x.to(device)\n",
    "        out = model(x)\n",
    "        print(out)\n",
    "        y = y.float().to(device)\n",
    "        print(y)\n",
    "        # print(torch.bmm(out, y)) \n",
    "        bvv = torch.bmm(out.view(2, 1, 5), y.view(2, 5, 1)).reshape(-1)\n",
    "        print(bvv)\n",
    "        print(bvv.size())\n",
    "        # print((out**2).sum(1, keepdim=True))\n",
    "        SE_out = torch.sqrt((out**2).sum(1, keepdim=False))\n",
    "        print(SE_out)\n",
    "        print(SE_out.size())\n",
    "        # print(out.size()[1])\n",
    "        \n",
    "        print(torch.cat((bvv, SE_out)))\n",
    "        \n",
    "        list_batch = [bvv, SE_out]\n",
    "        list_batch = torch.cat(list_batch)\n",
    "        print(list_batch)\n",
    "        \n",
    "        ori_weighting = 5 # relative to position weighting\n",
    "        MSE = nn.MSELoss(reduction='none')\n",
    "        # print(weighted_MSE_loss(out, y, ori_weighting))\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5311922a-bc81-4d52-95bb-5cd3781d1833",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = training_loop(model,optimizer,loss_function,num_epochs,train_lder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0e1b0f86-52a4-44a9-a753-84fa94bb5565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0587, device='cuda:0', dtype=torch.float64)\n",
      "tensor([[ 0.5505, -0.0080,  0.5124, -0.0217, -0.0615],\n",
      "        [ 0.5471,  0.0051,  0.5139, -0.0379,  0.0237]], device='cuda:0')\n",
      "tensor([[ 0.5764, -0.0043,  0.5101,  0.2047,  0.0008],\n",
      "        [ 0.5393,  0.0024,  0.5119, -0.0801,  0.0556]], device='cuda:0',\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i, (x, y) in enumerate(train_lder):\n",
    "        # print(x.size())\n",
    "        x = x.to(device)\n",
    "        out = trained_model(x)\n",
    "        y = y.to(device)\n",
    "        # out = out.to(device)\n",
    "        print(loss_function(out, y))\n",
    "        print(out)\n",
    "        print(y)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c305a7b3-db99-4552-ace8-699bfab3e8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5591, -0.0023,  0.5127,  0.0754, -0.0207],\n",
       "        [ 0.5488,  0.0055,  0.5150, -0.0272,  0.0273]], device='cuda:0')"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "bd92f0bb-c606-44ea-a3ca-fda7b15644af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_save_dir = os.getcwd()\n",
    "model_save_dir = '../../models'\n",
    "model_name = time.strftime(\"/model_%Y-%m-%d_%H-%M-%S.pt\")\n",
    "# model_name = '/test_model.pt'\n",
    "torch.save(trained_model.state_dict(), model_save_dir + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "914555ba-43e3-4587-9cd3-bfac0229caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trained_model.state_dict(), model_save_dir + model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e325534f-94dc-47d6-8f2e-dec801114b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ScrewingModel(\n",
       "  (lstm): LSTM(19, 38, num_layers=5, batch_first=True)\n",
       "  (hidden2out): Linear(in_features=38, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model = ScrewingModel(input_dim, hidden_dim, num_layers, output_dim)\n",
    "load_model.load_state_dict(torch.load(model_save_dir + model_name))\n",
    "load_model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
