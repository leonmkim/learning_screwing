{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff63794c-34e0-47db-b585-c801583699e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "bd4724fc-771f-4315-aee8-5a380a835558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "from bagpy import bagreader\n",
    "import rosbag\n",
    "import pandas as pd\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e5c6e5ba-f44f-401e-bcbd-25a97454f1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ep2j4fju'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.util.generate_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "545632d7-8d02-4a41-b5d7-636c1935de0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_id = '29ivurzi'\n",
    "# run = wandb.init(project=\"screwing_estimation\", entity=\"serialexperimentsleon\", group=group_id)\n",
    "# run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "335a71af-8576-4c49-aff7-a0110b011178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "fb507622-8aa7-4eb6-aad5-d50f8483640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from screwing_dataset import ScrewingDataset\n",
    "from screwing_model import ScrewingModel\n",
    "from screwing_model_seq import ScrewingModelSeq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "69f71876-1eef-46e3-a3ef-0ac6c93beaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "7b9179ce-1265-4689-a43a-ffd87c8a08e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "413ed73a-26e5-4d17-b49a-a37f5024626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dset_dir = os.path.expanduser('~/datasets/screwing')\n",
    "\n",
    "# xprmnt_dir = time.strftime(\"/%Y-%m-%d_%H-%M-%S\")\n",
    "xprmnt_dir = time.strftime(\"/2022-03-11_17-07-13\")\n",
    "# xprmnt_dir = time.strftime(\"/test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "9dec4fe9-1a1d-4352-9b70-5bb4de18b10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_path_names = base_dset_dir + xprmnt_dir + '/*.bag' \n",
    "\n",
    "bag_path_list = glob.glob(bag_path_names)\n",
    "total_num_dset = len(bag_path_list)\n",
    "num_dset = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "66ca807a-6b1f-4201-a201-6b4e566aba6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[2 0 7 6 9 5 3 4 8 1]\n",
      "6\n",
      "[2 0 7 6 9 5]\n",
      "[3 4 8 1]\n"
     ]
    }
   ],
   "source": [
    "bag_idxs = np.asarray(range(num_dset))\n",
    "rng = np.random.default_rng(2)\n",
    "\n",
    "print(bag_idxs)\n",
    "rng.shuffle(bag_idxs)\n",
    "# shuffled_bag_idxs = np.random.shuffle(bag_idxs)\n",
    "print(bag_idxs)\n",
    "\n",
    "train_prop = 0.6\n",
    "split = int(np.floor(train_prop * num_dset))\n",
    "\n",
    "print(split)\n",
    "train_bags = bag_idxs[:split]\n",
    "valid_bags = bag_idxs[split:]\n",
    "\n",
    "print(train_bags)\n",
    "print(valid_bags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "ed62ff9e-3298-458a-a30f-b6ee16af9c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/serialexperimentsleon/datasets/screwing/2022-03-11_17-07-13/4_2022-03-11-17-08-33.bag\n",
      "[INFO]  Data folder /home/serialexperimentsleon/datasets/screwing/2022-03-11_17-07-13/4_2022-03-11-17-08-33 already exists. Not creating.\n",
      "708\n"
     ]
    }
   ],
   "source": [
    "bag_path_names = base_dset_dir + xprmnt_dir + '/' + str(4) + '_*.bag' \n",
    "# print(bag_path_names)\n",
    "bag_path = glob.glob(bag_path_names)[0]\n",
    "print(bag_path)\n",
    "# bag = rosbag.Bag(bag_path)\n",
    "breader = bagreader(bag_path)\n",
    "bag = rosbag.Bag(bag_path)\n",
    "\n",
    "main_topic = '/panda/franka_state_controller_custom/franka_states'\n",
    "main_num_msgs = bag.get_message_count(main_topic)\n",
    "\n",
    "print(main_num_msgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e05f2a73-ca2c-4c37-9822-5d5fc4e3dfd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "708"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = breader.topic_table\n",
    "num_msgs = table['Message Count'][0]\n",
    "# table.loc[main_topic]\n",
    "# table\n",
    "num_msgs\n",
    "# type(num_msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "54f70b0c-c26d-4db8-82c4-706ce104a4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "708"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# idx = table.index[table['Topics'] == main_topic]\n",
    "num_msgs = table[table['Topics'] == main_topic]['Message Count'][0]\n",
    "num_msgs\n",
    "# row['Message Count'][0]\n",
    "# num_msgs = table.iloc[idx]['Message Count']\n",
    "# num_msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2a4c6eea-8f66-480c-98fe-f2b9e3dda48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.048346996307373"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = breader.start_time\n",
    "T = breader.end_time\n",
    "\n",
    "T - t0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7ee01620-1212-494e-94be-3790e0c3ac89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['/panda/franka_state_controller_custom/franka_states'])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breader.topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5a2eff8b-1281-4010-9793-5a5b5d185b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1647036513.459659\n",
      "1647036513.459659\n"
     ]
    }
   ],
   "source": [
    "main_topic_csv = breader.message_by_topic(main_topic)\n",
    "df = pd.read_csv(main_topic_csv)\n",
    "time = df.loc[0, 'Time']\n",
    "print(time)\n",
    "print(t0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4b9fc9aa-221c-4d16-93cf-d47760774252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.64703651e+09 1.64703651e+09 1.64703651e+09 1.64703651e+09\n",
      " 1.64703651e+09 1.64703651e+09]\n",
      "1647036537.508006\n"
     ]
    }
   ],
   "source": [
    "time = df.loc[0:5, 'Time'].to_numpy(dtype='float')\n",
    "print(time)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f8b318d8-32cf-4398-bb1e-3eda9b8feb2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.999985\n",
       "1    0.999985\n",
       "2    0.999985\n",
       "3    0.999985\n",
       "4    0.999986\n",
       "5    0.999986\n",
       "Name: O_T_EE_0, dtype: float64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose = df.loc[0:5, 'O_T_EE_0']\n",
    "pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0cfb1cba-2096-4fc5-b1e6-c609bc3a1435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.03378439, 0.06812978, 0.10193205, 0.13582706,\n",
       "       0.17023349])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "9af95dc1-7572-49f4-9674-f6703c1ef3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/serialexperimentsleon/datasets/screwing/2022-03-11_17-07-13/0_2022-03-11-17-07-18.bag\n",
      "[INFO]  Data folder /home/serialexperimentsleon/datasets/screwing/2022-03-11_17-07-13/0_2022-03-11-17-07-18 already exists. Not creating.\n"
     ]
    }
   ],
   "source": [
    "dset_list = []\n",
    "window_size = 10\n",
    "for i in range(num_dset):\n",
    "    id_str = str(i)\n",
    "    bag_path_names = base_dset_dir + xprmnt_dir + '/' + id_str + '_*.bag' \n",
    "    bag_path = glob.glob(bag_path_names)[0]\n",
    "    print(bag_path)\n",
    "\n",
    "    pos_path_name = base_dset_dir + xprmnt_dir + '/' + id_str + '_pos.npy'\n",
    "    proj_ori_path = base_dset_dir + xprmnt_dir + '/' + id_str + '_proj_ori.npy'\n",
    "    pos_ori_path_list = [pos_path_name, proj_ori_path]\n",
    "    \n",
    "    dset_list.append(ScrewingDataset(bag_path, pos_ori_path_list, window_size, overlapping=True))\n",
    "    \n",
    "concat_dset = ConcatDataset(dset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "e567b16a-c2e2-488e-9f6a-a7963680cce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387\n"
     ]
    }
   ],
   "source": [
    "length = len(concat_dset)\n",
    "print(length)\n",
    "train_size = int(.7*length)\n",
    "# train_size\n",
    "train_dset, test_dset = torch.utils.data.random_split(concat_dset, [train_size,length - train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "649a2772-b317-420a-8b0c-4ea3db8ec075",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "train_lder = DataLoader(\n",
    "    train_dset,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_lder = DataLoader(\n",
    "    test_dset,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "2f6b6f21-3e9b-4e41-8ae5-7cd98c9a726e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 19])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dset)\n",
    "x, y, t, T= train_dset[269]\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1d191260-f621-4347-8385-f33690ae1043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4042, 0.4076, 0.4109, 0.4142, 0.4174, 0.4174, 0.4210, 0.4219, 0.4249,\n",
      "         0.4281],\n",
      "        [0.0758, 0.0791, 0.0808, 0.0841, 0.0877, 0.0913, 0.0913, 0.0945, 0.0960,\n",
      "         0.0990]], dtype=torch.float64)\n",
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "for i,(x,y,t) in enumerate(train_lder):\n",
    "    print(t)\n",
    "    print(t.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "895938f5-66c7-4e0e-83d5-cfdee06c66cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 19\n",
    "hidden_dim = input_dim*2\n",
    "num_layers = int(window_size*0.5)\n",
    "output_dim = 5\n",
    "model = ScrewingModel(input_dim, hidden_dim, num_layers, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df30c12f-3508-4027-b517-cc9163825f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_metrics(model, ori_rel_weight, seq_length, val_loader): #TODO add early stopping criterion\n",
    "    # logging_step = 0\n",
    "    # quantiles of interest: median and 95% CI\n",
    "    q = torch.as_tensor([0.025, 0.5, 0.975]).to(device) \n",
    "    q_timing = torch.as_tensor([0.0, 0.2, 0.4, 0.6, 0.8, 1.0]).to(device)\n",
    "    seq_length = seq_length\n",
    "    ## switch model to eval\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for t in range(seq_length):\n",
    "\n",
    "            total_valid_pos_error, total_valid_ori_error, total_valid_loss = [], [], []\n",
    "            for batch_idx,(x,y, t) in enumerate(val_loader):\n",
    "                x = x.to(device)\n",
    "                y = y.float().to(device)\n",
    "\n",
    "                # Forward propogation happens here\n",
    "                outputs = model(x).to(device) \n",
    "                output_t = outputs[:, t, :]\n",
    "\n",
    "                loss = weighted_MSE_loss(output_t, y, ori_rel_weight)\n",
    "\n",
    "                ## evaluate and append analysis metrics\n",
    "                total_valid_ori_error.append(batched_ori_err(output_t, y))\n",
    "                total_valid_pos_error.append(batched_pos_err(output_t, y))\n",
    "                total_valid_loss.append(loss)\n",
    "\n",
    "                # if batch_idx % log_interval == 0:\n",
    "                #     wandb.log({\"loss\": loss, 'epoch': epoch, 'batch_idx': batch_idx})\n",
    "                \n",
    "                \n",
    "\n",
    "            total_valid_pos_error = torch.cat(total_valid_pos_error).to(device)\n",
    "            total_valid_ori_error = torch.cat(total_valid_ori_error).to(device)\n",
    "            total_valid_loss = torch.as_tensor(total_valid_loss).to(device)\n",
    "\n",
    "            ## statistical metrics from the test evaluations\n",
    "\n",
    "            ## pos error\n",
    "            pos_err_mean = torch.mean(total_valid_pos_error)\n",
    "            pos_err_std = torch.std(total_valid_pos_error)\n",
    "            pos_err_max = torch.max(total_valid_pos_error)\n",
    "            pos_err_min = torch.min(total_valid_pos_error)\n",
    "\n",
    "            ## 95% confidence interval and median\n",
    "            # q = torch.as_tensor([0.025, 0.5, 0.975]) \n",
    "            pos_err_95_median = torch.quantile(total_valid_pos_error, q, dim=0, keepdim=False, interpolation='nearest')\n",
    "\n",
    "            ## ori error\n",
    "            ori_err_mean = torch.mean(total_valid_ori_error)\n",
    "            ori_err_std = torch.std(total_valid_ori_error)\n",
    "            ori_err_max = torch.max(total_valid_ori_error)\n",
    "            ori_err_min = torch.min(total_valid_ori_error)\n",
    "\n",
    "            ## 95% confidence interval\n",
    "            ori_err_95_median = torch.quantile(total_valid_ori_error, q, dim=0, keepdim=False, interpolation='nearest')\n",
    "\n",
    "            ## loss \n",
    "            loss_mean = torch.mean(total_valid_loss)\n",
    "            loss_std = torch.std(total_valid_loss)\n",
    "            loss_max = torch.max(total_valid_loss)\n",
    "            loss_min = torch.min(total_valid_loss)\n",
    "\n",
    "            ## 95% confidence interval\n",
    "            loss_95_median = torch.quantile(total_valid_loss, q, dim=0, keepdim=False, interpolation='nearest')\n",
    "\n",
    "            wandb.log({ \n",
    "            'valid_pos_err_mean_' + str(t) : pos_err_mean,\n",
    "            'valid_pos_err_std_' + str(t) : pos_err_std,\n",
    "            'valid_pos_err_max_' + str(t) : pos_err_max,\n",
    "            'valid_pos_err_min_' + str(t) : pos_err_min,\n",
    "            'valid_pos_err_95_lower_' + str(t) : pos_err_95_median[0].item(),\n",
    "            'valid_pos_err_median_' + str(t) : pos_err_95_median[1].item(),\n",
    "            'valid_pos_err_95_upper_' + str(t) : pos_err_95_median[2].item(),\n",
    "            'valid_ori_err_mean_' + str(t) : ori_err_mean,\n",
    "            'valid_ori_err_std_' + str(t) : ori_err_std,\n",
    "            'valid_ori_err_max_' + str(t) : ori_err_max,\n",
    "            'valid_ori_err_min_' + str(t) : ori_err_min,\n",
    "            'valid_ori_err_95_lower_' + str(t) : ori_err_95_median[0].item(),\n",
    "            'valid_ori_err_median_' + str(t) : ori_err_95_median[1].item(),\n",
    "            'valid_ori_err_95_upper_' + str(t) : ori_err_95_median[2].item(),\n",
    "            'valid_loss_mean_' + str(t) : loss_mean,\n",
    "            'valid_loss_std_' + str(t) : loss_std,\n",
    "            'valid_loss_max_' + str(t) : loss_max,\n",
    "            'valid_loss_min_' + str(t) : loss_min,\n",
    "            'valid_loss_95_lower_' + str(t) : loss_95_median[0].item(),\n",
    "            'valid_loss_median_' + str(t) : loss_95_median[1].item(),\n",
    "            'valid_loss_95_upper_' + str(t) : loss_95_median[2].item()\n",
    "            }, step = logging_step-1)\n",
    "            ## log some summary metrics from the validation/eval run\n",
    "\n",
    "            ## log a figure of model output  \n",
    "\n",
    "        ## switch model back to train\n",
    "        model.train()\n",
    "             \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5c62e147-d1a2-4e71-b9f3-08ef7eb91acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(net,optimizer,criterion,num_epochs,train_loader):\n",
    "  overall_step = 0\n",
    "  counter = 0\n",
    "  loss_plot = []\n",
    "  for epoch in range(num_epochs):\n",
    "    for i,(x,y) in enumerate(train_loader):\n",
    "      counter += 1\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      x = x.to(device)\n",
    "      y = y.float().to(device)\n",
    "\n",
    "      # Forward propogation happens here\n",
    "      outputs = net(x).to(device) \n",
    "\n",
    "      # The following lines of code is where the magic happens!\n",
    "      # The partial derivatives of the Loss with respect to the \n",
    "      # weights are calculated and the weights are updated\n",
    "      loss = criterion(outputs,y)  \n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      loss_plot.append(loss.item())\n",
    "\n",
    "      # if (counter+1) % 10 == 0: \n",
    "          # argmax = torch.max(outputs, 1)[1]\n",
    "          # accuracy = (y == argmax.squeeze()).float().mean()\n",
    "\n",
    "#           info = {'loss' : loss.item()}\n",
    "#           for tag, value in info.items():\n",
    "#               logger.scalar_summary(tag, value, overall_step)\n",
    "#           overall_step+=1\n",
    "\n",
    "#           print ('Epoch : %d/%d, Iter : %d/%d,  Loss: %.4f' \n",
    "#                  %(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
    "  return net #, loss_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "8a3f0881-48cb-44a4-96f6-dd79e70f8e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 0.003\n",
    "\n",
    "# net = My_Neural_Network()\n",
    "# net = net.to(device)\n",
    "\n",
    "# model = ScrewingModel(input_dim, hidden_dim, ouput_dim) # no specification of \n",
    "model = model.to(device)\n",
    "loss_function = nn.MSELoss(reduction='sum') #alternatively mean the squared error or none ...\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "8ae244e2-ee66-4447-9035-dd9e86bfab16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4243, 0.1488, 0.7550, 0.0404, 0.7973],\n",
      "        [0.7016, 0.0033, 0.2402, 0.8475, 0.3636]])\n",
      "tensor([1.1872, 1.1833])\n",
      "tensor([[1.],\n",
      "        [1.]])\n",
      "tensor([[0.9657, 0.3413, 0.5083, 0.4488, 0.7348],\n",
      "        [0.2556, 0.4523, 0.4474, 0.1619, 0.5287]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4394, 0.4359, 1.4852, 0.0901, 1.0850],\n",
       "        [2.7447, 0.0073, 0.5368, 5.2353, 0.6876]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2,5)\n",
    "print(a)\n",
    "print(torch.norm(a, 2, 1))\n",
    "ones = torch.ones(2, 1)\n",
    "print(ones)\n",
    "torch.cat((a, ones), 1)\n",
    "\n",
    "b  = torch.rand(2,5)\n",
    "print(b)\n",
    "a / b\n",
    "# a.size()[0]\n",
    "# len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cf19125f-3697-4150-a4c6-95fb810f7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_MSE_loss(input, target, ori_weight):\n",
    "    SE_out = (input - target)**2\n",
    "    # print(torch.mean(SE_out))\n",
    "    # print(torch.sum(SE_out)/torch.numel(input))\n",
    "    pos_SE = SE_out[:, :3]\n",
    "    ori_SE = SE_out[:, 3:]\n",
    "    # print((torch.sum(pos_SE) + torch.sum(ori_SE))/torch.numel(input))\n",
    "    weighted_mean = (torch.sum(pos_SE) + ori_weight*torch.sum(ori_SE))/len(SE_out)\n",
    "    # weighted_sum = torch.sum(pos_SE) + ori_weight*torch.sum(ori_SE)\n",
    "    return weighted_mean\n",
    "    # return torch.sum(torch.mul(weighting, MSE_out))\n",
    "    # return torch.sum(MSE_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8648e3d2-b3d5-4d82-8f09-ad97ca7cebc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0254, -0.1393,  0.1538, -0.1316,  0.0819],\n",
      "        [ 0.0253, -0.1405,  0.1519, -0.1305,  0.0827]], device='cuda:0')\n",
      "tensor([[ 0.5526, -0.0125,  0.5124, -0.0292, -0.0315],\n",
      "        [ 0.5529, -0.0057,  0.5125, -0.0111, -0.0009]], device='cuda:0')\n",
      "tensor([0.0959, 0.0940], device='cuda:0')\n",
      "torch.Size([2])\n",
      "tensor([0.2603, 0.2594], device='cuda:0')\n",
      "torch.Size([2])\n",
      "tensor([0.0959, 0.0940, 0.2603, 0.2594], device='cuda:0')\n",
      "tensor([0.0959, 0.0940, 0.2603, 0.2594], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i, (x, y) in enumerate(train_lder):\n",
    "        # print(x.size())\n",
    "        x = x.to(device)\n",
    "        out = model(x)\n",
    "        print(out)\n",
    "        y = y.float().to(device)\n",
    "        print(y)\n",
    "        # print(torch.bmm(out, y)) \n",
    "        bvv = torch.bmm(out.view(2, 1, 5), y.view(2, 5, 1)).reshape(-1)\n",
    "        print(bvv)\n",
    "        print(bvv.size())\n",
    "        # print((out**2).sum(1, keepdim=True))\n",
    "        SE_out = torch.sqrt((out**2).sum(1, keepdim=False))\n",
    "        print(SE_out)\n",
    "        print(SE_out.size())\n",
    "        # print(out.size()[1])\n",
    "        \n",
    "        print(torch.cat((bvv, SE_out)))\n",
    "        \n",
    "        list_batch = [bvv, SE_out]\n",
    "        list_batch = torch.cat(list_batch)\n",
    "        print(list_batch)\n",
    "        \n",
    "        ori_weighting = 5 # relative to position weighting\n",
    "        MSE = nn.MSELoss(reduction='none')\n",
    "        # print(weighted_MSE_loss(out, y, ori_weighting))\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5311922a-bc81-4d52-95bb-5cd3781d1833",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = training_loop(model,optimizer,loss_function,num_epochs,train_lder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0e1b0f86-52a4-44a9-a753-84fa94bb5565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0587, device='cuda:0', dtype=torch.float64)\n",
      "tensor([[ 0.5505, -0.0080,  0.5124, -0.0217, -0.0615],\n",
      "        [ 0.5471,  0.0051,  0.5139, -0.0379,  0.0237]], device='cuda:0')\n",
      "tensor([[ 0.5764, -0.0043,  0.5101,  0.2047,  0.0008],\n",
      "        [ 0.5393,  0.0024,  0.5119, -0.0801,  0.0556]], device='cuda:0',\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i, (x, y) in enumerate(train_lder):\n",
    "        # print(x.size())\n",
    "        x = x.to(device)\n",
    "        out = trained_model(x)\n",
    "        y = y.to(device)\n",
    "        # out = out.to(device)\n",
    "        print(loss_function(out, y))\n",
    "        print(out)\n",
    "        print(y)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c305a7b3-db99-4552-ace8-699bfab3e8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5591, -0.0023,  0.5127,  0.0754, -0.0207],\n",
       "        [ 0.5488,  0.0055,  0.5150, -0.0272,  0.0273]], device='cuda:0')"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "bd92f0bb-c606-44ea-a3ca-fda7b15644af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_save_dir = os.getcwd()\n",
    "model_save_dir = '../../models'\n",
    "model_name = time.strftime(\"/model_%Y-%m-%d_%H-%M-%S.pt\")\n",
    "# model_name = '/test_model.pt'\n",
    "torch.save(trained_model.state_dict(), model_save_dir + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "914555ba-43e3-4587-9cd3-bfac0229caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trained_model.state_dict(), model_save_dir + model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e325534f-94dc-47d6-8f2e-dec801114b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ScrewingModel(\n",
       "  (lstm): LSTM(19, 38, num_layers=5, batch_first=True)\n",
       "  (hidden2out): Linear(in_features=38, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model = ScrewingModel(input_dim, hidden_dim, num_layers, output_dim)\n",
    "load_model.load_state_dict(torch.load(model_save_dir + model_name))\n",
    "load_model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
